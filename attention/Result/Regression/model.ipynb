{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports & Configuration",
   "id": "89cc2476cc73f479"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from brevitas.nn import QuantIdentity\n",
    "from reservoirpy.nodes import Reservoir, Ridge\n",
    "from reservoirpy import Node\n",
    "from reservoirpy.datasets import narma\n",
    "from reservoirpy.observables import rmse, nrmse, rsquare\n",
    "import os\n",
    "\n",
    "# ── Configuration ─────────────────────────────────────\n",
    "N                     = 800        # Size: Total number of \"brain cells\" in the reservoir\n",
    "RIDGE                 = 1e-10      # Penalty: Prevents the output weights from getting too extreme (overfitting)\n",
    "SR                    = 0.5        # Memory Life: Higher SR (0.9) needed for NARMA-30's 30-step memory\n",
    "INPUT_SCALING         = 0.7        # Strength: Reduced to avoid saturation with longer memory chains\n",
    "INPUT_CONNECTIVITY    = 0.9        # Entry: % of neurons that are directly wired to the input source\n",
    "RC_CONNECTIVITY       = 0.2        # Network: % of neurons that are wired to each other (internal wiring)\n",
    "LR                    = 1.0        # Speed: How fast the neurons react to new info (1.0 = instant update)\n",
    "WARMUP                = 1000       # Settling: Steps to ignore at the start to let the \"echoes\" stabilize\n",
    "SEED                  = 2341       # Consistency: Ensures you get the same random network every time you run it\n",
    "\n",
    "\n",
    "# NARMA dataset\n",
    "N_TIMESTEPS           = 10000      # Data Length: Total seconds/steps of data available\n",
    "NARMA_ORDER           = 20\n",
    "TRAIN_LEN             = 5000       # Training: Number of samples used to teach the output layer\n",
    "\n",
    "DATASET_TAG           = f\"NARMA{NARMA_ORDER}\"\n",
    "\n",
    "QUANTIZATION_BITS_LIST = [4, 6, 8]\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Dataset:           {DATASET_TAG}\")\n",
    "print(f\"  Reservoir neurons: {N}\")\n",
    "print(f\"  NARMA order:       {NARMA_ORDER}\")\n",
    "print(f\"  Training length:   {TRAIN_LEN}\")\n",
    "print(f\"  Test length:       {N_TIMESTEPS - TRAIN_LEN}\")\n"
   ],
   "id": "62f04ee414e475fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter Search (Don't Run Now)",
   "id": "6109d3a936a3a8d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "RUN_HYPERPARAM_SEARCH = True  # Set to True to run hyperparameter search\n",
    "\n",
    "if RUN_HYPERPARAM_SEARCH:\n",
    "    import pandas as pd\n",
    "    from itertools import product\n",
    "    from time import time\n",
    "    import warnings\n",
    "\n",
    "    # Suppress ill-conditioned matrix warnings for cleaner output\n",
    "    warnings.filterwarnings('ignore', message='Ill-conditioned matrix')\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"  ADVANCED HYPERPARAMETER SEARCH\")\n",
    "    print(\"  Fixed: N=500 | Tests: FP32 + 6-bit + 8-bit\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    def setup_quant_params(num_bits):\n",
    "        return {\n",
    "            'bits': num_bits,\n",
    "            'min_val': -(2 ** (num_bits - 1)),\n",
    "            'max_val': (2 ** (num_bits - 1)) - 1,\n",
    "            'threshold_scale': 1.0 / (2 ** num_bits),\n",
    "            'div_scale': 2 ** num_bits\n",
    "        }\n",
    "\n",
    "    def extract_Qinput(array, num_bits):\n",
    "        quant_id = QuantIdentity(return_quant_tensor=True, bit_width=num_bits)\n",
    "        t = torch.tensor(array, dtype=torch.float32)\n",
    "        qt = quant_id(t)\n",
    "        return qt.int().detach().numpy(), qt.scale.detach().numpy(), qt.zero_point.detach().numpy()\n",
    "\n",
    "    def compute_integer_thresholds(scale):\n",
    "        return np.int32(-1 / scale), np.int32(1 / scale)\n",
    "\n",
    "    def piecewise_linear_hard_tanh_integer(x, lo, hi, div_scale):\n",
    "        x = np.clip(x, lo, hi)\n",
    "        x = x + hi\n",
    "        return (x / div_scale).astype(np.int32)\n",
    "\n",
    "    # Define hyperparameter search space\n",
    "    N_FIXED = 500\n",
    "    search_space = {\n",
    "        'SR':              [0.5, 0.7, 0.9],          # Spectral radius\n",
    "        'RIDGE':           [1e-10, 1e-8, 1e-6],      # Ridge regularization\n",
    "        'LR':              [0.5, 0.7, 1.0],          # Leak rate\n",
    "        'INPUT_SCALING':   [0.5, 0.7, 1.0],          # Input scaling\n",
    "        'RC_CONNECTIVITY': [0.05, 0.1, 0.2]          # Reservoir connectivity (NEW)\n",
    "    }\n",
    "\n",
    "    # Generate all combinations\n",
    "    param_names = list(search_space.keys())\n",
    "    param_values = list(search_space.values())\n",
    "    combinations = list(product(*param_values))\n",
    "\n",
    "    total_runs = len(combinations) * 3  # FP32 + 6bit + 8bit\n",
    "    print(f\"\\nTesting {len(combinations)} hyperparameter combinations × 3 models = {total_runs} runs\")\n",
    "    print(f\"Fixed: N={N_FIXED}, INPUT_CONNECTIVITY={INPUT_CONNECTIVITY}, WARMUP={WARMUP}, SEED={SEED}\\n\")\n",
    "    print(\"This will take ~10-15 minutes...\\n\")\n",
    "\n",
    "    # Prepare dataset (same for all runs)\n",
    "    rng = np.random.default_rng(seed=SEED)\n",
    "    u_search = rng.uniform(0, 0.5, size=(N_TIMESTEPS + NARMA_ORDER, 1))\n",
    "    X_narma_search = narma(n_timesteps=N_TIMESTEPS, order=NARMA_ORDER, u=u_search)\n",
    "    X_search = X_narma_search[0] if isinstance(X_narma_search, tuple) else X_narma_search\n",
    "    X_search = X_search[:N_TIMESTEPS]\n",
    "\n",
    "    X_train_search = u_search[NARMA_ORDER:TRAIN_LEN + NARMA_ORDER]\n",
    "    y_train_search = X_search[1:TRAIN_LEN + 1]\n",
    "    X_test_search = u_search[TRAIN_LEN + NARMA_ORDER + 1:-1]\n",
    "    y_test_search = X_search[TRAIN_LEN + 2:]\n",
    "\n",
    "    # Helper function to quantize and evaluate\n",
    "    def evaluate_quantized(esn_fp, num_bits, ridge_val):\n",
    "        \"\"\"Quantize FP32 ESN and evaluate.\"\"\"\n",
    "        try:\n",
    "            qp = setup_quant_params(num_bits)\n",
    "\n",
    "            # Quantize weights and inputs\n",
    "            int_x, x_scale, _ = extract_Qinput(u_search, num_bits)\n",
    "            int_Win, scale_Win, _ = extract_Qinput(esn_fp.nodes[0].Win.todense(), num_bits)\n",
    "            int_Wr, scale_Wr, _ = extract_Qinput(esn_fp.nodes[0].W.todense(), num_bits)\n",
    "\n",
    "            bias_raw = esn_fp.nodes[0].bias\n",
    "            bias_array = np.full((1, N_FIXED), bias_raw) if np.isscalar(bias_raw) else np.asarray(bias_raw.todense())\n",
    "            int_bias, _, _ = extract_Qinput(bias_array, num_bits)\n",
    "\n",
    "            # Collapse scales to scalars\n",
    "            if hasattr(x_scale, 'shape') and x_scale.size > 1: x_scale = np.mean(x_scale)\n",
    "            if hasattr(scale_Win, 'shape') and scale_Win.size > 1: scale_Win = np.mean(scale_Win)\n",
    "            if hasattr(scale_Wr, 'shape') and scale_Wr.size > 1: scale_Wr = np.mean(scale_Wr)\n",
    "\n",
    "            input_scale = scale_Win * x_scale\n",
    "            reservoir_scale = scale_Wr * qp['threshold_scale']\n",
    "            inp_lo, inp_hi = compute_integer_thresholds(input_scale)\n",
    "            res_lo, res_hi = compute_integer_thresholds(reservoir_scale)\n",
    "\n",
    "            # Create quantized node\n",
    "            class QuantNode(Node):\n",
    "                def __init__(self, Wr, Win, bias, lo_i, hi_i, lo_r, hi_r, div):\n",
    "                    self.Wr_int = Wr\n",
    "                    self.Win_int = Win\n",
    "                    self.Bias_int = bias.flatten()\n",
    "                    self.inp_lo = lo_i\n",
    "                    self.inp_hi = hi_i\n",
    "                    self.res_lo = lo_r\n",
    "                    self.res_hi = hi_r\n",
    "                    self.div_scale = div\n",
    "                    self.output_dim = N_FIXED\n",
    "                    self.input_dim = None\n",
    "                    self.initialized = False\n",
    "\n",
    "                def initialize(self, x, y=None):\n",
    "                    self.input_dim = x.shape[-1]\n",
    "                    self.state = {\"out\": np.zeros((N_FIXED,), dtype=np.int64)}\n",
    "                    self.initialized = True\n",
    "\n",
    "                def _step(self, state, x):\n",
    "                    s = state[\"out\"].astype(np.int64).reshape(1, N_FIXED)\n",
    "                    recurrent = s @ self.Wr_int.astype(np.int32)\n",
    "                    inp = x.reshape(1, -1) @ self.Win_int.astype(np.int32).T\n",
    "                    out_inp = piecewise_linear_hard_tanh_integer(inp, self.inp_lo, self.inp_hi, self.div_scale)\n",
    "                    out_rec = piecewise_linear_hard_tanh_integer(recurrent, self.res_lo, self.res_hi, self.div_scale)\n",
    "                    return {\"out\": (out_inp + out_rec + self.Bias_int.reshape(1, N_FIXED)).flatten()}\n",
    "\n",
    "            node = QuantNode(int_Wr, int_Win, int_bias, inp_lo, inp_hi, res_lo, res_hi, qp['div_scale'])\n",
    "\n",
    "            # Train readout on quantized states\n",
    "            int_x_train = int_x[NARMA_ORDER:TRAIN_LEN + NARMA_ORDER]\n",
    "            states_train = node.run(int_x_train.astype(np.float64)) * qp['threshold_scale']\n",
    "\n",
    "            readout = Ridge(ridge=ridge_val)\n",
    "            readout.fit(states_train, y_train_search, warmup=WARMUP)\n",
    "\n",
    "            # Evaluate on test set\n",
    "            int_x_test = int_x[TRAIN_LEN + NARMA_ORDER + 1:-1]\n",
    "            states_test = node.run(int_x_test.astype(np.float64)) * qp['threshold_scale']\n",
    "            y_pred = readout.run(states_test)\n",
    "\n",
    "            return {\n",
    "                'rmse': rmse(y_test_search, y_pred),\n",
    "                'nrmse': nrmse(y_test_search, y_pred),\n",
    "                'r2': rsquare(y_test_search, y_pred)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'rmse': np.nan, 'nrmse': np.nan, 'r2': np.nan}\n",
    "\n",
    "    # Run search\n",
    "    results = []\n",
    "    start_time = time()\n",
    "    run_count = 0\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        # Unpack parameters\n",
    "        sr_val, ridge_val, lr_val, input_scaling_val, rc_conn_val = params\n",
    "\n",
    "        # ========== FP32 Baseline ==========\n",
    "        run_count += 1\n",
    "        try:\n",
    "            reservoir_fp = Reservoir(\n",
    "                units=N_FIXED,\n",
    "                lr=lr_val,\n",
    "                sr=sr_val,\n",
    "                input_connectivity=INPUT_CONNECTIVITY,\n",
    "                rc_connectivity=rc_conn_val,\n",
    "                input_scaling=input_scaling_val,\n",
    "                seed=SEED\n",
    "            )\n",
    "            readout_fp = Ridge(ridge=ridge_val)\n",
    "            esn_fp = reservoir_fp >> readout_fp\n",
    "            esn_fp.fit(X_train_search, y_train_search, warmup=WARMUP)\n",
    "\n",
    "            y_pred_fp = esn_fp.run(X_test_search)\n",
    "            rmse_fp = rmse(y_test_search, y_pred_fp)\n",
    "            nrmse_fp = nrmse(y_test_search, y_pred_fp)\n",
    "            r2_fp = rsquare(y_test_search, y_pred_fp)\n",
    "        except:\n",
    "            rmse_fp, nrmse_fp, r2_fp = np.nan, np.nan, np.nan\n",
    "            esn_fp = None\n",
    "\n",
    "        results.append({\n",
    "            'N': N_FIXED,\n",
    "            'SR': sr_val,\n",
    "            'RIDGE': ridge_val,\n",
    "            'LR': lr_val,\n",
    "            'INPUT_SCALING': input_scaling_val,\n",
    "            'RC_CONNECTIVITY': rc_conn_val,\n",
    "            'MODEL': 'FP32',\n",
    "            'RMSE': rmse_fp,\n",
    "            'NRMSE': nrmse_fp,\n",
    "            'R2': r2_fp\n",
    "        })\n",
    "\n",
    "        # ========== 6-bit Quantization ==========\n",
    "        run_count += 1\n",
    "        if esn_fp is not None:\n",
    "            metrics_6 = evaluate_quantized(esn_fp, 6, ridge_val)\n",
    "        else:\n",
    "            metrics_6 = {'rmse': np.nan, 'nrmse': np.nan, 'r2': np.nan}\n",
    "\n",
    "        results.append({\n",
    "            'N': N_FIXED,\n",
    "            'SR': sr_val,\n",
    "            'RIDGE': ridge_val,\n",
    "            'LR': lr_val,\n",
    "            'INPUT_SCALING': input_scaling_val,\n",
    "            'RC_CONNECTIVITY': rc_conn_val,\n",
    "            'MODEL': '6-bit',\n",
    "            'RMSE': metrics_6['rmse'],\n",
    "            'NRMSE': metrics_6['nrmse'],\n",
    "            'R2': metrics_6['r2']\n",
    "        })\n",
    "\n",
    "        # ========== 8-bit Quantization ==========\n",
    "        run_count += 1\n",
    "        if esn_fp is not None:\n",
    "            metrics_8 = evaluate_quantized(esn_fp, 8, ridge_val)\n",
    "        else:\n",
    "            metrics_8 = {'rmse': np.nan, 'nrmse': np.nan, 'r2': np.nan}\n",
    "\n",
    "        results.append({\n",
    "            'N': N_FIXED,\n",
    "            'SR': sr_val,\n",
    "            'RIDGE': ridge_val,\n",
    "            'LR': lr_val,\n",
    "            'INPUT_SCALING': input_scaling_val,\n",
    "            'RC_CONNECTIVITY': rc_conn_val,\n",
    "            'MODEL': '8-bit',\n",
    "            'RMSE': metrics_8['rmse'],\n",
    "            'NRMSE': metrics_8['nrmse'],\n",
    "            'R2': metrics_8['r2']\n",
    "        })\n",
    "\n",
    "        # Progress update\n",
    "        if i % 10 == 0 or i == len(combinations):\n",
    "            elapsed = time() - start_time\n",
    "            eta = (elapsed / run_count) * (total_runs - run_count)\n",
    "            r2_6_str = f\"{metrics_6['r2']:.3f}\" if not np.isnan(metrics_6['r2']) else \"nan\"\n",
    "            r2_8_str = f\"{metrics_8['r2']:.3f}\" if not np.isnan(metrics_8['r2']) else \"nan\"\n",
    "            print(f\"  [{run_count}/{total_runs}] SR={sr_val:.1f}, RIDGE={ridge_val:.0e}, RC_CONN={rc_conn_val:.2f} | FP32 R²={r2_fp:.3f}, 6bit R²={r2_6_str}, 8bit R²={r2_8_str} | ETA: {eta:.0f}s\")\n",
    "\n",
    "    elapsed = time() - start_time\n",
    "    print(f\"\\n Search completed in {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv('hyperparam_search_advanced.csv', index=False)\n",
    "    print(f\" Full results saved to: hyperparam_search_advanced.csv\")\n",
    "\n",
    "    # Display best for each model type\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  BEST CONFIGURATION PER MODEL TYPE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    for model_type in ['FP32', '6-bit', '8-bit']:\n",
    "        df_model = df_results[df_results['MODEL'] == model_type].copy()\n",
    "        df_model = df_model.dropna(subset=['R2'])  # Remove failed runs\n",
    "        df_model = df_model.sort_values('R2', ascending=False)\n",
    "\n",
    "        if len(df_model) > 0:\n",
    "            best = df_model.iloc[0]\n",
    "            print(f\"\\n{model_type}:\")\n",
    "            print(f\"  N={int(best['N'])}, SR={best['SR']}, RIDGE={best['RIDGE']:.0e}, LR={best['LR']}, IN_SCALE={best['INPUT_SCALING']}, RC_CONN={best['RC_CONNECTIVITY']}\")\n",
    "            print(f\"  → RMSE={best['RMSE']:.6f}, NRMSE={best['NRMSE']:.6f}, R²={best['R2']:.6f}\")\n",
    "\n",
    "            # Save to file\n",
    "            with open(f'best_config_{model_type.replace(\"-\", \"\")}.txt', 'w') as f:\n",
    "                f.write(f\"# Best {model_type} Configuration\\n\")\n",
    "                f.write(f\"N = {int(best['N'])}\\n\")\n",
    "                f.write(f\"SR = {best['SR']}\\n\")\n",
    "                f.write(f\"RIDGE = {best['RIDGE']:.0e}\\n\")\n",
    "                f.write(f\"LR = {best['LR']}\\n\")\n",
    "                f.write(f\"INPUT_SCALING = {best['INPUT_SCALING']}\\n\")\n",
    "                f.write(f\"INPUT_CONNECTIVITY = {INPUT_CONNECTIVITY}\\n\")\n",
    "                f.write(f\"RC_CONNECTIVITY = {best['RC_CONNECTIVITY']}\\n\")\n",
    "                f.write(f\"\\n# Performance\\n\")\n",
    "                f.write(f\"RMSE = {best['RMSE']:.6f}\\n\")\n",
    "                f.write(f\"NRMSE = {best['NRMSE']:.6f}\\n\")\n",
    "                f.write(f\"R2 = {best['R2']:.6f}\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" Best configs saved to: best_config_FP32.txt, best_config_6bit.txt, best_config_8bit.txt\")\n",
    "\n",
    "else:\n",
    "    print(\"Hyperparameter search disabled. Set RUN_HYPERPARAM_SEARCH = True to enable.\")\n"
   ],
   "id": "14703775f049ba9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load & Prepare NARMA Dataset\n",
    "\n",
    "NARMA (Nonlinear Auto-Regressive Moving Average) is a classic benchmark for reservoir computing."
   ],
   "id": "4dd7e6863a2965d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Generating {DATASET_TAG} dataset...\")\n",
    "\n",
    "# Generate random input signal\n",
    "rng = np.random.default_rng(seed=SEED)\n",
    "u = rng.uniform(0, 0.5, size=(N_TIMESTEPS + NARMA_ORDER, 1))\n",
    "\n",
    "# Generate NARMA target signal\n",
    "X_narma = narma(n_timesteps=N_TIMESTEPS, order=NARMA_ORDER, u=u)\n",
    "\n",
    "# Check if narma returns tuple or array\n",
    "if isinstance(X_narma, tuple):\n",
    "    X = X_narma[0]\n",
    "else:\n",
    "    X = X_narma\n",
    "\n",
    "# Ensure X has exactly N_TIMESTEPS elements\n",
    "X = X[:N_TIMESTEPS]\n",
    "\n",
    "# Prepare train/test splits\n",
    "X_train = u[NARMA_ORDER:TRAIN_LEN + NARMA_ORDER]   # Input for training\n",
    "y_train = X[1:TRAIN_LEN + 1]                       # Target for training (offset by 1 for prediction)\n",
    "\n",
    "X_test = u[TRAIN_LEN + NARMA_ORDER + 1:-1]         # Input for testing\n",
    "y_test = X[TRAIN_LEN + 2:]                         # Target for testing\n",
    "\n",
    "print(f\"\\nDataset prepared:\")\n",
    "print(f\"  Full signal length: {len(X)}\")\n",
    "print(f\"  X_train: {X_train.shape}  (input signal)\")\n",
    "print(f\"  y_train: {y_train.shape}  (target signal)\")\n",
    "print(f\"  X_test:  {X_test.shape}\")\n",
    "print(f\"  y_test:  {y_test.shape}\")\n",
    "\n",
    "assert X_test.shape[0] == y_test.shape[0], f\"Test shapes mismatch: X_test {X_test.shape} != y_test {y_test.shape}\"\n",
    "print(f\"   Test shapes match!\")\n",
    "\n",
    "print(f\"\\n  Input range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "print(f\"  Target range: [{y_train.min():.3f}, {y_train.max():.3f}]\")\n"
   ],
   "id": "e20d82b8843da4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bqz45mzx4ti",
   "source": [
    "# Save input signal\n",
    "np.save(f\"u_{DATASET_TAG}.npy\", u)\n",
    "print(f\"Input signal saved: u_{DATASET_TAG}.npy  {u.shape}\")\n"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ── Quantization utilities ────────────────────────────\n",
    "def setup_quant_params(num_bits):\n",
    "    \"\"\"Integer-range constants for a given bit-width.\"\"\"\n",
    "    return {\n",
    "        'bits':             num_bits,\n",
    "        'min_val':          -(2 ** (num_bits - 1)),\n",
    "        'max_val':          (2 ** (num_bits - 1)) - 1,\n",
    "        'threshold_scale':  1.0 / (2 ** num_bits),\n",
    "        'div_scale':        2 ** num_bits\n",
    "    }\n",
    "\n",
    "def extract_Qinput(array, num_bits):\n",
    "    \"\"\"Quantize a numpy array to num_bits integers via Brevitas.\"\"\"\n",
    "    quant_id = QuantIdentity(return_quant_tensor=True, bit_width=num_bits)\n",
    "    t  = torch.tensor(array, dtype=torch.float32)\n",
    "    qt = quant_id(t)\n",
    "    return qt.int().detach().numpy(), qt.scale.detach().numpy(), qt.zero_point.detach().numpy()\n",
    "\n",
    "def compute_integer_thresholds(scale):\n",
    "    \"\"\"Return (lo, hi) hard-tanh bounds in integer domain.\"\"\"\n",
    "    return np.int32(-1 / scale), np.int32(1 / scale)\n",
    "\n",
    "def piecewise_linear_hard_tanh_integer(x, lo, hi, div_scale):\n",
    "    \"\"\"Quantized hard-tanh: clip → shift → integer divide.\"\"\"\n",
    "    x = np.clip(x, lo, hi)\n",
    "    x = x + hi                          # shift [lo, hi] → [0, 2*hi]\n",
    "    return (x / div_scale).astype(np.int32)"
   ],
   "id": "b2e1b09908aa3af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train FP32 Baseline ESN",
   "id": "20b19b9cd269a356"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Training FP32 ESN baseline...\")\n",
    "\n",
    "reservoir_fp32 = Reservoir(\n",
    "    units=N,\n",
    "    lr=LR,\n",
    "    sr=SR,\n",
    "    input_connectivity=INPUT_CONNECTIVITY,\n",
    "    rc_connectivity=RC_CONNECTIVITY,\n",
    "    input_scaling=INPUT_SCALING,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "readout_fp32 = Ridge(ridge=RIDGE)\n",
    "esn_fp32 = reservoir_fp32 >> readout_fp32\n",
    "esn_fp32 = esn_fp32.fit(X_train, y_train, warmup=WARMUP)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_fp32 = esn_fp32.run(X_test)\n",
    "\n",
    "fp32_rmse = rmse(y_test, y_pred_fp32)\n",
    "fp32_nrmse = nrmse(y_test, y_pred_fp32)\n",
    "fp32_r2 = rsquare(y_test, y_pred_fp32)\n",
    "\n",
    "print(f\"\\nFP32 Baseline Results:\")\n",
    "print(f\"  RMSE:   {fp32_rmse:.6f}\")\n",
    "print(f\"  NRMSE:  {fp32_nrmse:.6f}\")\n",
    "print(f\"  R²:     {fp32_r2:.6f}\")"
   ],
   "id": "c6b91582ae11ab09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Quantized ESN — Build, Train, Evaluate",
   "id": "78f81bc6f049213e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_quantized_esn(num_bits, esn_fp32, X_train, y_train, X_test, y_test, u_full):\n",
    "    \"\"\"\n",
    "    Quantize the trained FP32 ESN to num_bits and evaluate.\n",
    "    Returns metrics, node, readout, and states.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  {num_bits}-bit Quantization\")\n",
    "    print(f\"{'='*50}\")\n",
    "    qp = setup_quant_params(num_bits)\n",
    "\n",
    "    # Quantize inputs and weights\n",
    "    print(\"  Quantizing inputs and weights...\")\n",
    "    int_x,      x_scale,   _ = extract_Qinput(u_full,                              num_bits)\n",
    "    int_Win,    scale_Win, _ = extract_Qinput(esn_fp32.nodes[0].Win.todense(),     num_bits)\n",
    "    int_Wr,     scale_Wr,  _ = extract_Qinput(esn_fp32.nodes[0].W.todense(),       num_bits)\n",
    "\n",
    "    # Handle bias (can be scalar or array)\n",
    "    bias_raw = esn_fp32.nodes[0].bias\n",
    "    bias_array = np.full((1, N), bias_raw) if np.isscalar(bias_raw) else np.asarray(bias_raw.todense())\n",
    "    int_bias,   _,         _ = extract_Qinput(bias_array,                          num_bits)\n",
    "\n",
    "    # Collapse scales to scalars\n",
    "    if hasattr(x_scale,   'shape') and x_scale.size   > 1: x_scale   = np.mean(x_scale)\n",
    "    if hasattr(scale_Win, 'shape') and scale_Win.size > 1: scale_Win = np.mean(scale_Win)\n",
    "    if hasattr(scale_Wr,  'shape') and scale_Wr.size  > 1: scale_Wr  = np.mean(scale_Wr)\n",
    "\n",
    "    input_scale     = scale_Win * x_scale\n",
    "    reservoir_scale = scale_Wr  * qp['threshold_scale']\n",
    "\n",
    "    # Integer thresholds\n",
    "    inp_lo, inp_hi = compute_integer_thresholds(input_scale)\n",
    "    res_lo, res_hi = compute_integer_thresholds(reservoir_scale)\n",
    "    print(f\"  Input thresholds:     [{inp_lo}, {inp_hi}]\")\n",
    "    print(f\"  Reservoir thresholds: [{res_lo}, {res_hi}]\")\n",
    "\n",
    "    # Build quantized reservoir node\n",
    "    class QuantizedNode(Node):\n",
    "        def __init__(self, name=None):\n",
    "            self.name        = name\n",
    "            self.output_dim  = N\n",
    "            self.input_dim   = None\n",
    "            self.initialized = False\n",
    "\n",
    "        def initialize(self, x, y=None):\n",
    "            self.input_dim   = x.shape[-1]\n",
    "            self.output_dim  = N\n",
    "            self.Wr          = int_Wr\n",
    "            self.Win         = int_Win\n",
    "            self.Bias        = int_bias.flatten()\n",
    "            self.state       = {\"out\": np.zeros((N,), dtype=np.int64)}\n",
    "            self.initialized = True\n",
    "\n",
    "        def _step(self, state, x):\n",
    "            s         = state[\"out\"].astype(np.int64).reshape(1, N)\n",
    "            recurrent = s @ self.Wr.astype(np.int32)\n",
    "            inp       = x.reshape(1, -1) @ self.Win.astype(np.int32).T\n",
    "            out_inp   = piecewise_linear_hard_tanh_integer(inp,       inp_lo, inp_hi, qp['div_scale'])\n",
    "            out_rec   = piecewise_linear_hard_tanh_integer(recurrent, res_lo, res_hi, qp['div_scale'])\n",
    "            next_out  = (out_inp + out_rec + self.Bias.reshape(1, N)).flatten()\n",
    "            return {\"out\": next_out}\n",
    "\n",
    "    quant_node = QuantizedNode(name=f\"quant_reservoir_{num_bits}bit\")\n",
    "\n",
    "    # Train readout on quantized states\n",
    "    print(\"  Running reservoir on training data...\")\n",
    "    int_x_train = int_x[NARMA_ORDER:TRAIN_LEN + NARMA_ORDER]\n",
    "    states_train = quant_node.run(int_x_train.astype(np.float64)) * qp['threshold_scale']\n",
    "\n",
    "    print(\"  Fitting Ridge readout...\")\n",
    "    quant_readout = Ridge(ridge=RIDGE)\n",
    "    quant_readout.fit(states_train, y_train, warmup=WARMUP)\n",
    "\n",
    "    # Evaluate on test set (match the indexing used for X_test in FP32)\n",
    "    print(\"  Running reservoir on test data...\")\n",
    "    int_x_test = int_x[TRAIN_LEN + NARMA_ORDER + 1:-1]  # Added +1 to match X_test indexing\n",
    "    states_test = quant_node.run(int_x_test.astype(np.float64)) * qp['threshold_scale']\n",
    "    y_pred = quant_readout.run(states_test)\n",
    "\n",
    "    # Metrics\n",
    "    test_rmse = rmse(y_test, y_pred)\n",
    "    test_nrmse = nrmse(y_test, y_pred)\n",
    "    test_r2 = rsquare(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n  RMSE:   {test_rmse:.6f}\")\n",
    "    print(f\"  NRMSE:  {test_nrmse:.6f}\")\n",
    "    print(f\"  R²:     {test_r2:.6f}\")\n",
    "\n",
    "    return {\n",
    "        'metrics': {\n",
    "            'rmse': test_rmse,\n",
    "            'nrmse': test_nrmse,\n",
    "            'r2': test_r2\n",
    "        },\n",
    "        'node': quant_node,\n",
    "        'readout': quant_readout,\n",
    "        'int_Wr': int_Wr,\n",
    "        'int_Win': int_Win,\n",
    "        'int_bias': int_bias,\n",
    "        'states_train': states_train,\n",
    "        'states_test': states_test,\n",
    "        'quant_params': qp\n",
    "    }"
   ],
   "id": "39f1bb02e0aa42c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run All Quantization Levels",
   "id": "621d12231cf4788b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "quantized_results = {}\n",
    "\n",
    "for bits in QUANTIZATION_BITS_LIST:\n",
    "    quantized_results[bits] = run_quantized_esn(\n",
    "        bits, esn_fp32, X_train, y_train, X_test, y_test, u\n",
    "    )\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'Model':<10} {'RMSE':<15} {'NRMSE':<15} {'R²':<15}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'FP32':<10} {fp32_rmse:<15.6f} {fp32_nrmse:<15.6f} {fp32_r2:<15.6f}\")\n",
    "for bits in QUANTIZATION_BITS_LIST:\n",
    "    m = quantized_results[bits]['metrics']\n",
    "    print(f\"{f'{bits}-bit':<10} {m['rmse']:<15.6f} {m['nrmse']:<15.6f} {m['r2']:<15.6f}\")\n",
    "print(\"=\"*70)"
   ],
   "id": "720505ed6ae1ad1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save Quantized Weights",
   "id": "8fec9bdf7bccd4d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "for bits in QUANTIZATION_BITS_LIST:\n    wdir = os.path.join(\"weights\", f\"{bits}bit\")\n    os.makedirs(wdir, exist_ok=True)\n\n    r = quantized_results[bits]\n    np.save(os.path.join(wdir, f\"quantized_reservoir_weights_Wr_{DATASET_TAG}_{bits}bit.npy\"),  r['int_Wr'])\n    np.save(os.path.join(wdir, f\"quantized_input_weights_Win_{DATASET_TAG}_{bits}bit.npy\"),     r['int_Win'])\n    np.save(os.path.join(wdir, f\"quantized_bias_weights_{DATASET_TAG}_{bits}bit.npy\"),          r['int_bias'])\n    np.save(os.path.join(wdir, f\"readout_weights_Wout_{DATASET_TAG}_{bits}bit.npy\"),            r['readout'].Wout)\n    np.save(os.path.join(wdir, f\"readout_bias_{DATASET_TAG}_{bits}bit.npy\"),                    r['readout'].bias)\n\n    # Save quantization scales (needed for re-running with pruned weights)\n    qp_save = setup_quant_params(bits)\n    _, x_scale_save,   _ = extract_Qinput(u,                              bits)\n    _, scale_Win_save, _ = extract_Qinput(esn_fp32.nodes[0].Win.todense(), bits)\n    _, scale_Wr_save,  _ = extract_Qinput(esn_fp32.nodes[0].W.todense(),   bits)\n\n    if hasattr(x_scale_save,   'shape') and x_scale_save.size   > 1: x_scale_save   = np.mean(x_scale_save)\n    if hasattr(scale_Win_save, 'shape') and scale_Win_save.size > 1: scale_Win_save = np.mean(scale_Win_save)\n    if hasattr(scale_Wr_save,  'shape') and scale_Wr_save.size  > 1: scale_Wr_save  = np.mean(scale_Wr_save)\n\n    np.save(os.path.join(wdir, f\"scale_x_{bits}bit.npy\"),   x_scale_save)\n    np.save(os.path.join(wdir, f\"scale_Win_{bits}bit.npy\"), scale_Win_save)\n    np.save(os.path.join(wdir, f\"scale_Wr_{bits}bit.npy\"),  scale_Wr_save)\n\n    print(f\"  {bits}-bit weights and scales saved → {wdir}/\")\n\nprint(f\"\\nAll weights saved for {DATASET_TAG}.\")\n",
   "id": "f95763653fd5587",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Save States for Attention Pruning\n",
    "\n",
    "**Important:** Save reservoir states for later use in attention-based neuron importance scoring."
   ],
   "id": "2f8748f96944dd36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "states_dir = \"states\"\nos.makedirs(states_dir, exist_ok=True)\n\nfor bits in QUANTIZATION_BITS_LIST:\n    np.save(os.path.join(states_dir, f\"states_train_{bits}bit.npy\"), quantized_results[bits][\"states_train\"])\n    np.save(os.path.join(states_dir, f\"states_test_{bits}bit.npy\"),  quantized_results[bits][\"states_test\"])\n\n# Targets are the same regardless of quantization level\nnp.save(os.path.join(states_dir, \"y_train.npy\"), y_train)\nnp.save(os.path.join(states_dir, \"y_test.npy\"),  y_test)\n\nprint(f\"States saved for {DATASET_TAG}:\")\nfor bits in QUANTIZATION_BITS_LIST:\n    print(f\"  {bits}-bit: states_train_{bits}bit.npy, states_test_{bits}bit.npy\")\nprint(\"  y_train.npy, y_test.npy\")\n",
   "id": "b265f1de2237bd65",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
