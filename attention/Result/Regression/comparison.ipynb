{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## why Attention Based ?\n",
    "In standard neural networks, \"attention\" learns to focus on important data by continuously updating weights during training. However, in Reservoir Computing, the internal connections are completely fixed. Because we cannot learn attention, the EC-Var method acts as a built-in attention mechanism by observing how data naturally flows through the network. It scores every connection by looking at three things: how physically strong the connection is (|W|), how well it passes information from one time step to the next (lagged correlation), and how active the receiving neuron is (standard deviation). By pruning away the low-scoring connections, EC-Var structurally forces the network to \"pay attention\" only to the most active and useful memory pathways. This achieves the exact goal of an attention mechanism—focusing the network's capacity on what matters most—without breaking the rules of a fixed-weight reservoir."
   ],
   "id": "16f80f5c67975333"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Regression Pruning Method Comparison\n",
    "This notebook compares pruning methods on a regression model using saved weights and states.\n"
   ],
   "id": "fd3b9dd9f5b96c48"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Imports and configuration.",
   "id": "a7d273954ac9ec1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mutual_info_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from reservoirpy.nodes import Ridge\n",
    "from reservoirpy import Node\n",
    "from reservoirpy.observables import rmse, nrmse, rsquare\n",
    "import torch\n",
    "from brevitas.nn import QuantIdentity\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# ── Configuration ──────────────────────────────────────────────────────────────\n",
    "DATASET_DIR       = \".\"\n",
    "NARMA_ORDER       = 20\n",
    "DATASET_TAG       = f\"NARMA{NARMA_ORDER}\"\n",
    "DATASET_NAME      = DATASET_TAG\n",
    "\n",
    "N_NEURONS         = 800\n",
    "QUANTIZATION_BITS = 4\n",
    "SEED              = 2341\n",
    "WARMUP            = 1000\n",
    "RIDGE             = 1e-10\n",
    "\n",
    "TRAIN_LEN         = 5000\n",
    "N_TIMESTEPS       = 10000\n",
    "\n",
    "SPARSITY_LEVELS   = [0.0, 0.15, 0.30, 0.45, 0.60, 0.70]\n",
    "\n",
    "MAX_WEIGHTS_TO_TEST = 500\n",
    "MI_BINS           = QUANTIZATION_BITS * 2\n",
    "LASSO_ALPHA       = 1e-5\n",
    "\n",
    "RETRAIN_READOUT   = True\n",
    "RUN_MI            = True\n",
    "\n",
    "seed = SEED\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Dataset: {DATASET_TAG} | {QUANTIZATION_BITS}-bit | N={N_NEURONS}\")\n"
   ],
   "id": "ae2940a016c38779",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load model artifacts (states, targets, weights, scales, input signal).",
   "id": "bc993c29bd3a635f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "states_dir  = os.path.join(DATASET_DIR, \"states\")\n",
    "weights_dir = os.path.join(DATASET_DIR, \"weights\", f\"{QUANTIZATION_BITS}bit\")\n",
    "\n",
    "states_train = np.load(os.path.join(states_dir, f\"states_train_{QUANTIZATION_BITS}bit.npy\"))\n",
    "states_test  = np.load(os.path.join(states_dir, f\"states_test_{QUANTIZATION_BITS}bit.npy\"))\n",
    "y_train      = np.load(os.path.join(states_dir, \"y_train.npy\"))\n",
    "y_test       = np.load(os.path.join(states_dir, \"y_test.npy\"))\n",
    "\n",
    "W_res        = np.load(os.path.join(weights_dir, f\"quantized_reservoir_weights_Wr_{DATASET_TAG}_{QUANTIZATION_BITS}bit.npy\"))\n",
    "W_in         = np.load(os.path.join(weights_dir, f\"quantized_input_weights_Win_{DATASET_TAG}_{QUANTIZATION_BITS}bit.npy\"))\n",
    "W_out        = np.load(os.path.join(weights_dir, f\"readout_weights_Wout_{DATASET_TAG}_{QUANTIZATION_BITS}bit.npy\"))\n",
    "readout_bias = np.load(os.path.join(weights_dir, f\"readout_bias_{DATASET_TAG}_{QUANTIZATION_BITS}bit.npy\"))\n",
    "int_bias     = np.load(os.path.join(weights_dir, f\"quantized_bias_weights_{DATASET_TAG}_{QUANTIZATION_BITS}bit.npy\"))\n",
    "\n",
    "x_scale      = np.load(os.path.join(weights_dir, f\"scale_x_{QUANTIZATION_BITS}bit.npy\"))\n",
    "scale_Win    = np.load(os.path.join(weights_dir, f\"scale_Win_{QUANTIZATION_BITS}bit.npy\"))\n",
    "scale_Wr     = np.load(os.path.join(weights_dir, f\"scale_Wr_{QUANTIZATION_BITS}bit.npy\"))\n",
    "\n",
    "# Input signal\n",
    "u = np.load(os.path.join(DATASET_DIR, f\"u_{DATASET_TAG}.npy\"))\n",
    "\n",
    "# Rebuild input splits for quantized runs\n",
    "X_train = u[NARMA_ORDER:TRAIN_LEN + NARMA_ORDER]\n",
    "X_test  = u[TRAIN_LEN + NARMA_ORDER + 1:-1]\n",
    "\n",
    "# EC/PCA use post-warmup states\n",
    "if WARMUP > 0:\n",
    "    states_train_ec = states_train[WARMUP:]\n",
    "    y_train_ec      = y_train[WARMUP:]\n",
    "else:\n",
    "    states_train_ec = states_train\n",
    "    y_train_ec      = y_train\n",
    "\n",
    "print(f\"Loaded {DATASET_TAG} artifacts from '{DATASET_DIR}'\")\n",
    "print(f\"  W_res: {W_res.shape}  |  states_train: {states_train.shape}  |  u: {u.shape}\")\n"
   ],
   "id": "df6d018ebc362667",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Baseline performance with saved readout.",
   "id": "7ad68318e40c556d"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "readout_baseline = Ridge(ridge=RIDGE)\nreadout_baseline.output_dim = 1\nreadout_baseline.input_dim = N_NEURONS\nreadout_baseline.Wout = W_out\nreadout_baseline.bias = readout_bias\nreadout_baseline.state = {\"out\": np.zeros((1,))}\nreadout_baseline.initialized = True\n\n# Ensure 2D target shape for metric helpers\nif y_test.ndim == 1:\n    y_test_eval = y_test.reshape(-1, 1)\nelse:\n    y_test_eval = y_test\n\nbaseline_pred = readout_baseline.run(states_test)\nif baseline_pred.ndim == 1:\n    baseline_pred = baseline_pred.reshape(-1, 1)\n\nbaseline_rmse = rmse(y_test_eval, baseline_pred)\nbaseline_nrmse = nrmse(y_test_eval, baseline_pred)\nbaseline_r2 = rsquare(y_test_eval, baseline_pred)\n\nprint(\"Baseline metrics\")\nprint(f\"RMSE: {baseline_rmse:.6f}\")\nprint(f\"NRMSE: {baseline_nrmse:.6f}\")\nprint(f\"R2: {baseline_r2:.6f}\")\n"
   ],
   "id": "ba14384f2acba6c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Quantization helpers and quantized reservoir node.",
   "id": "23bb66feec6ab92"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def setup_quant_params(num_bits):\n    return {\n        'bits': num_bits,\n        'min_val': -(2 ** (num_bits - 1)),\n        'max_val': (2 ** (num_bits - 1)) - 1,\n        'threshold_scale': 1.0 / (2 ** num_bits),\n        'div_scale': 2 ** num_bits\n    }\n\ndef extract_Qinput(array, num_bits):\n    quant_id = QuantIdentity(return_quant_tensor=True, bit_width=num_bits)\n    t = torch.tensor(array, dtype=torch.float32)\n    qt = quant_id(t)\n    return qt.int().detach().numpy(), qt.scale.detach().numpy(), qt.zero_point.detach().numpy()\n\ndef compute_integer_thresholds(scale):\n    return np.int32(-1 / scale), np.int32(1 / scale)\n\ndef piecewise_linear_hard_tanh_integer(x, lo, hi, div_scale):\n    x = np.clip(x, lo, hi)\n    x = x + hi\n    return (x / div_scale).astype(np.int32)\n\nqp = setup_quant_params(QUANTIZATION_BITS)\n\nint_x, _, _ = extract_Qinput(u, QUANTIZATION_BITS)\nint_x_train = int_x[NARMA_ORDER:TRAIN_LEN + NARMA_ORDER]\nint_x_test = int_x[TRAIN_LEN + NARMA_ORDER + 1:-1]\n\ninput_scale = scale_Win * x_scale\nreservoir_scale = scale_Wr * qp['threshold_scale']\n\ninp_lo, inp_hi = compute_integer_thresholds(input_scale)\nres_lo, res_hi = compute_integer_thresholds(reservoir_scale)\n\nclass QuantizedReservoirNode(Node):\n    def __init__(self, Wr_matrix, Win_matrix, bias_array, name=None):\n        self.name = name\n        self.output_dim = N_NEURONS\n        self.input_dim = None\n        self.initialized = False\n        self.Wr = Wr_matrix\n        self.Win = Win_matrix\n        self.Bias = bias_array.flatten()\n\n    def initialize(self, x, y=None):\n        self.input_dim = x.shape[-1]\n        self.state = {\"out\": np.zeros((N_NEURONS,), dtype=np.int64)}\n        self.initialized = True\n\n    def _step(self, state, x):\n        s = state[\"out\"].astype(np.int64).reshape(1, N_NEURONS)\n        recurrent = s @ self.Wr.astype(np.int32)\n        inp = x.reshape(1, -1) @ self.Win.astype(np.int32).T\n        out_inp = piecewise_linear_hard_tanh_integer(inp, inp_lo, inp_hi, qp['div_scale'])\n        out_rec = piecewise_linear_hard_tanh_integer(recurrent, res_lo, res_hi, qp['div_scale'])\n        next_out = (out_inp + out_rec + self.Bias.reshape(1, N_NEURONS)).flatten()\n        return {\"out\": next_out}\n\n    def get_param(self, name):\n        if name == \"Wr\":\n            return self.Wr\n        if name == \"Win\":\n            return self.Win\n        if name == \"Bias\":\n            return self.Bias\n        raise KeyError(name)\n\n    def set_param(self, name, value):\n        if name == \"Wr\":\n            self.Wr = value\n            return\n        if name == \"Win\":\n            self.Win = value\n            return\n        if name == \"Bias\":\n            self.Bias = value\n            return\n        raise KeyError(name)\n\nquant_node = QuantizedReservoirNode(W_res, W_in, int_bias, name=\"quant_reservoir\")\n"
   ],
   "id": "df47cab8a43f8dc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Shared metrics helpers.",
   "id": "21801d69a495920"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_all_metrics_multi_dim(y_true, y_pred):\n    metrics = {}\n    n_outputs = y_true.shape[1]\n    dim_metrics = {'mse': [], 'mae': [], 'rmse': [], 'nrmse': [], 'r2': []}\n\n    for dim in range(n_outputs):\n        y_true_dim = y_true[:, dim]\n        y_pred_dim = y_pred[:, dim]\n\n        dim_mse = mean_squared_error(y_true_dim, y_pred_dim)\n        dim_mae = mean_absolute_error(y_true_dim, y_pred_dim)\n\n        dim_rmse = rmse(y_true_dim, y_pred_dim)\n        dim_nrmse = nrmse(y_true_dim, y_pred_dim)\n        dim_r2 = rsquare(y_true_dim, y_pred_dim)\n\n        dim_metrics['mse'].append(dim_mse)\n        dim_metrics['mae'].append(dim_mae)\n        dim_metrics['rmse'].append(dim_rmse)\n        dim_metrics['nrmse'].append(dim_nrmse)\n        dim_metrics['r2'].append(dim_r2)\n\n    for metric_name in dim_metrics:\n        metrics[metric_name] = np.mean(dim_metrics[metric_name])\n\n    metrics['dim_metrics'] = dim_metrics\n    return metrics\n"
   ],
   "id": "2070ba7cd721e670",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Quantized baseline metrics for fixed readout.",
   "id": "eef9e2467d07475f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "quantized_node_fixed = QuantizedReservoirNode(W_res, W_in, int_bias, name=\"quant_reservoir_fixed\")\nquantized_readout = copy.deepcopy(readout_baseline)\n\nQuantized_States_test = quantized_node_fixed.run(int_x_test.astype(np.float64)) * qp['threshold_scale']\ny_pred_quantized = quantized_readout.run(Quantized_States_test)\nif y_pred_quantized.ndim == 1:\n    y_pred_quantized = y_pred_quantized.reshape(-1, 1)\n\nquantized_metrics = calculate_all_metrics_multi_dim(y_test_eval, y_pred_quantized)\n\n"
   ],
   "id": "8a8cacb2a2d264b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## EC-Var Score Computation\n\nThe lagged correlation matrix is computed **inside** `compute_ec_var_scores()` — it is private to the EC method and not exposed as a loose variable.\n\n**EC-Var formula:** `Score[i,j] = |W_res[i,j]| × |Corr(xᵢ(t), xⱼ(t-1))| × std(xᵢ(t))`\n- `|W_res[i,j]|` → connection is physically strong\n- `|Corr(xᵢ(t), xⱼ(t-1))|` → real information flows through it (lagged = causal/temporal)\n- `std(xᵢ(t))` → destination neuron i is expressive and active",
   "id": "5939392d4970cc28"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "W_res_float = W_res.astype(np.float64)   # shared by LASSO and other methods below\n",
    "\n",
    "def compute_ec_var_scores(states, W_res, N_NEURONS):\n",
    "    \"\"\"\n",
    "    EC-Var scoring function.\n",
    "    Score[i,j] = |W[i,j]| × |Corr(x_i(t), x_j(t-1))| × std(x_i(t))\n",
    "    \"\"\"\n",
    "    # Lagged rank correlation: how much does neuron j(t-1) predict neuron i(t)?\n",
    "    lagged_correlation = np.zeros((N_NEURONS, N_NEURONS))\n",
    "    for i in range(N_NEURONS):\n",
    "        for j in range(N_NEURONS):\n",
    "            lagged_correlation[i, j] = spearmanr(states[1:, i], states[:-1, j]).correlation\n",
    "    lagged_correlation = np.nan_to_num(lagged_correlation, nan=0.0)\n",
    "\n",
    "    # Destination neuron expressiveness\n",
    "    dest_std      = states.std(axis=0)\n",
    "    dest_std_safe = np.where(dest_std == 0, 1.0, dest_std)\n",
    "\n",
    "    return (\n",
    "        np.abs(W_res.astype(np.float64))\n",
    "        * np.abs(lagged_correlation)\n",
    "        * dest_std_safe.reshape(N_NEURONS, 1)\n",
    "    )\n",
    "\n",
    "effective_connectivity = compute_ec_var_scores(states_train_ec, W_res, N_NEURONS)\n",
    "\n",
    "print(f\"EC-Var score statistics (active connections):\")\n",
    "print(f\"  Mean:   {effective_connectivity[W_res != 0].mean():.4f}\")\n",
    "print(f\"  Median: {np.median(effective_connectivity[W_res != 0]):.4f}\")\n",
    "print(f\"  Std:    {effective_connectivity[W_res != 0].std():.4f}\")\n"
   ],
   "id": "1ae3b9ffb5001dda",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Method 1 — EC-Var (Effective Connectivity with Variance)\n\n**Score:** `|W[i,j]| × |SpearmanCorr(xᵢ(t), xⱼ(t-1))| × std(xᵢ(t))`\n\nThree signals combined into one pruning score:\n- **Weight magnitude** — the connection is physically strong\n- **Lagged rank correlation** — information actually flows through it (causal, temporal)\n- **Destination std** — the receiving neuron is active and expressive, not silent\n\nConnections that score low on any of these three dimensions are pruned first.\nReadout is retrained after each pruning step for a fair evaluation.",
   "id": "9afdee8354fcf879"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def prune_by_effective_connectivity(W_res, connectivity_scores, sparsity):\n    active_mask = (W_res != 0)\n    active_indices = np.argwhere(active_mask)\n    n_active = active_indices.shape[0]\n\n    n_keep_active = int(n_active * (1 - sparsity))\n    n_keep_active = max(0, min(n_keep_active, n_active))\n\n    mask = np.zeros_like(W_res, dtype=bool)\n    if n_keep_active == 0 or n_active == 0:\n        return W_res * mask, mask\n\n    active_scores = connectivity_scores[active_mask]\n    top_idx = np.argsort(active_scores)[-n_keep_active:]\n    kept_active_indices = active_indices[top_idx]\n    mask[kept_active_indices[:, 0], kept_active_indices[:, 1]] = True\n\n    W_pruned = W_res * mask\n    return W_pruned, mask\n\nattention_results = {\n    'removal_percentage': [],\n    'rmse_values': [],\n    'r2_values': []\n}\n\nfor sparsity in SPARSITY_LEVELS:\n    W_res_ec, _ = prune_by_effective_connectivity(W_res, effective_connectivity, sparsity)\n    node = QuantizedReservoirNode(W_res_ec, W_in, int_bias, name=\"quant_reservoir_ec\")\n\n    if RETRAIN_READOUT:\n        states_train_pruned = node.run(int_x_train.astype(np.float64)) * qp['threshold_scale']\n        readout_use = Ridge(ridge=RIDGE)\n        readout_use.fit(states_train_pruned, y_train, warmup=WARMUP)\n    else:\n        readout_use = readout_baseline\n\n    states_test_pruned = node.run(int_x_test.astype(np.float64)) * qp['threshold_scale']\n    y_pred = readout_use.run(states_test_pruned)\n    if y_pred.ndim == 1:\n        y_pred = y_pred.reshape(-1, 1)\n\n    metrics = calculate_all_metrics_multi_dim(y_test_eval, y_pred)\n    attention_results['removal_percentage'].append(int(sparsity * 100))\n    attention_results['rmse_values'].append(metrics['rmse'])\n    attention_results['r2_values'].append(metrics['r2'])\n"
   ],
   "id": "103fdcb3838bfeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 — Random Pruning (Lower-Bound Baseline)\n",
    "\n",
    "**Score:** None — connections are removed uniformly at random.\n",
    "\n",
    "Every non-zero weight has equal probability of being pruned, with no knowledge of its importance. Results are averaged over 5 independent trials to reduce variance.\n",
    "\n",
    "Serves as the **lower-bound baseline**"
   ],
   "id": "c10d29416d7343dc"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def random_weight_removal_analysis_quantized(W_quantized, custom_node, X_train, y_train,\n                                             X_test, y_test, threshold_scale,\n                                             removal_percentages, num_trials=5, random_seed=seed):\n    np.random.seed(random_seed)\n    random.seed(random_seed)\n\n    non_zero_mask = W_quantized != 0\n    non_zero_positions = list(zip(*np.where(non_zero_mask)))\n    total_nonzero_weights = len(non_zero_positions)\n\n    random_results = {\n        'removal_percentage': [],\n        'weights_removed': [],\n        'mse_values': [],\n        'mae_values': [],\n        'rmse_values': [],\n        'nrmse_values': [],\n        'r2_values': [],\n        'mse_std': [],\n        'mae_std': [],\n        'rmse_std': [],\n        'nrmse_std': [],\n        'r2_std': []\n    }\n\n    for removal_pct in removal_percentages:\n        num_weights_to_remove = int((removal_pct / 100) * total_nonzero_weights)\n\n        trial_results = {'mse': [], 'mae': [], 'rmse': [], 'nrmse': [], 'r2': []}\n\n        for trial in range(num_trials):\n            W_modified = W_quantized.copy()\n            if num_weights_to_remove > 0:\n                weights_to_remove = random.sample(non_zero_positions, num_weights_to_remove)\n                for row, col in weights_to_remove:\n                    W_modified[row, col] = 0\n\n            modified_node = copy.deepcopy(custom_node)\n            modified_node.set_param(\"Wr\", W_modified)\n\n            states_train_pruned = modified_node.run(X_train.astype(np.float64)) * threshold_scale\n            retrained_readout = Ridge(ridge=RIDGE)\n            retrained_readout.fit(states_train_pruned, y_train, warmup=WARMUP)\n\n            quantized_states_test = modified_node.run(X_test.astype(np.float64)) * threshold_scale\n            y_pred_modified = retrained_readout.run(quantized_states_test)\n            if y_pred_modified.ndim == 1:\n                y_pred_modified = y_pred_modified.reshape(-1, 1)\n            modified_metrics = calculate_all_metrics_multi_dim(y_test, y_pred_modified)\n\n            for metric in trial_results.keys():\n                trial_results[metric].append(modified_metrics[metric])\n\n        random_results['removal_percentage'].append(removal_pct)\n        random_results['weights_removed'].append(num_weights_to_remove)\n\n        for metric in trial_results.keys():\n            finite_values = [val for val in trial_results[metric] if not (math.isinf(val) or math.isnan(val))]\n            if finite_values:\n                avg_val = np.mean(finite_values)\n                std_val = np.std(finite_values) if len(finite_values) > 1 else 0.0\n            else:\n                avg_val = float('inf') if metric != 'r2' else -float('inf')\n                std_val = 0.0\n            random_results[f'{metric}_values'].append(avg_val)\n            random_results[f'{metric}_std'].append(std_val)\n\n    return random_results\n\nremoval_percentages = [int(s * 100) for s in SPARSITY_LEVELS]\n\nrandom_results = random_weight_removal_analysis_quantized(\n    W_res, quantized_node_fixed, int_x_train, y_train,\n    int_x_test, y_test_eval, qp['threshold_scale'],\n    removal_percentages=removal_percentages,\n    num_trials=5, random_seed=seed\n)\n",
   "id": "7c1ece1fe60cd0b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Method 3 — PCA Pruning\n\n**Score:** PCA loading magnitude of the destination neuron on the top principal component.\n\nFits PCA on the reservoir training states. Neurons with high loadings on the first principal component encode the most variance in the reservoir dynamics and are considered important. All connections *into* a neuron inherit that neuron's importance as their score.\n\n**Unsupervised** — no task signal (y) is used, only the structure of the reservoir states.",
   "id": "278fbde21c6bc07f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca_components = 1\n",
    "\n",
    "pca = PCA(n_components=pca_components)\n",
    "pca.fit(states_train_ec)\n",
    "\n",
    "neuron_importance = np.sum(np.abs(pca.components_), axis=0)\n",
    "\n",
    "weight_scores_pca = np.tile(neuron_importance.reshape(1, -1), (W_res.shape[0], 1))\n",
    "\n",
    "\n",
    "def prune_by_pca_scores(W_res, weight_scores, sparsity):\n",
    "    active_mask = (W_res != 0)\n",
    "    active_indices = np.argwhere(active_mask)\n",
    "    n_active = active_indices.shape[0]\n",
    "\n",
    "    n_keep_active = int(n_active * (1 - sparsity))\n",
    "    n_keep_active = max(0, min(n_keep_active, n_active))\n",
    "\n",
    "    mask = np.zeros_like(W_res, dtype=bool)\n",
    "    if n_keep_active == 0 or n_active == 0:\n",
    "        return W_res * mask, mask\n",
    "\n",
    "    active_scores = weight_scores[active_mask]\n",
    "    top_idx = np.argsort(active_scores)[-n_keep_active:]\n",
    "    kept_active_indices = active_indices[top_idx]\n",
    "    mask[kept_active_indices[:, 0], kept_active_indices[:, 1]] = True\n",
    "\n",
    "    W_pruned = W_res * mask\n",
    "    return W_pruned, mask\n",
    "\n",
    "pca_results = {\n",
    "    'removal_percentage': [],\n",
    "    'rmse_values': [],\n",
    "    'r2_values': []\n",
    "}\n",
    "\n",
    "for sparsity in SPARSITY_LEVELS:\n",
    "    W_res_pca, _ = prune_by_pca_scores(W_res, weight_scores_pca, sparsity)\n",
    "    node = QuantizedReservoirNode(W_res_pca, W_in, int_bias, name=\"quant_reservoir_pca\")\n",
    "\n",
    "    if RETRAIN_READOUT:\n",
    "        states_train_pruned = node.run(int_x_train.astype(np.float64)) * qp['threshold_scale']\n",
    "        readout_use = Ridge(ridge=RIDGE)\n",
    "        readout_use.fit(states_train_pruned, y_train, warmup=WARMUP)\n",
    "    else:\n",
    "        readout_use = readout_baseline\n",
    "\n",
    "    states_test_pruned = node.run(int_x_test.astype(np.float64)) * qp['threshold_scale']\n",
    "    y_pred = readout_use.run(states_test_pruned)\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred.reshape(-1, 1)\n",
    "\n",
    "    metrics = calculate_all_metrics_multi_dim(y_test_eval, y_pred)\n",
    "    pca_results['removal_percentage'].append(int(sparsity * 100))\n",
    "    pca_results['rmse_values'].append(metrics['rmse'])\n",
    "    pca_results['r2_values'].append(metrics['r2'])\n",
    "\n"
   ],
   "id": "9c5f95b540702ea3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4 — LASSO Pruning\n",
    "\n",
    "**Score:** `|W[i,j]| × |lasso_coeff[i]|`\n",
    "\n",
    "Fits an L1-regularized (LASSO) regression from training reservoir states to the target signal. The L1 penalty shrinks unimportant neuron coefficients to exactly zero. Connections into neurons that LASSO zeroed out are pruned first; connections into task-relevant neurons are protected.\n",
    "\n",
    "**Supervised** — directly uses the target signal (y) to identify important neurons."
   ],
   "id": "c52e8e1fb23cd3bb"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fit LASSO on post-warmup training states to get neuron importance\n",
    "lasso_model = Lasso(alpha=LASSO_ALPHA, max_iter=10000, random_state=SEED)\n",
    "lasso_model.fit(states_train_ec, y_train_ec.ravel())\n",
    "\n",
    "lasso_coeff = np.abs(lasso_model.coef_)\n",
    "lasso_coeff_safe = np.where(lasso_coeff == 0, 0.0, lasso_coeff)\n",
    "\n",
    "n_nonzero = np.sum(lasso_coeff > 0)\n",
    "print(f\"LASSO (alpha={LASSO_ALPHA}): {n_nonzero}/{N_NEURONS} neurons have non-zero coefficients\")\n",
    "print(f\"  coeff range: [{lasso_coeff.min():.6f}, {lasso_coeff.max():.6f}]\")\n",
    "\n",
    "# score[i,j] = |W_res[i,j]| × |lasso_coeff[i]|\n",
    "lasso_scores = np.abs(W_res_float) * lasso_coeff_safe.reshape(N_NEURONS, 1)\n",
    "\n",
    "lasso_results = {\n",
    "    'removal_percentage': [],\n",
    "    'rmse_values': [],\n",
    "    'r2_values': []\n",
    "}\n",
    "\n",
    "removal_percentages = [int(s * 100) for s in SPARSITY_LEVELS]\n",
    "\n",
    "for sparsity in SPARSITY_LEVELS:\n",
    "    W_res_lasso, _ = prune_by_effective_connectivity(W_res, lasso_scores, sparsity)\n",
    "    node = QuantizedReservoirNode(W_res_lasso, W_in, int_bias, name=\"quant_reservoir_lasso\")\n",
    "\n",
    "    states_train_pruned = node.run(int_x_train.astype(np.float64)) * qp['threshold_scale']\n",
    "    readout_use = Ridge(ridge=RIDGE)\n",
    "    readout_use.fit(states_train_pruned, y_train, warmup=WARMUP)\n",
    "\n",
    "    states_test_pruned = node.run(int_x_test.astype(np.float64)) * qp['threshold_scale']\n",
    "    y_pred = readout_use.run(states_test_pruned)\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred.reshape(-1, 1)\n",
    "\n",
    "    metrics = calculate_all_metrics_multi_dim(y_test_eval, y_pred)\n",
    "    lasso_results['removal_percentage'].append(int(sparsity * 100))\n",
    "    lasso_results['rmse_values'].append(metrics['rmse'])\n",
    "    lasso_results['r2_values'].append(metrics['r2'])\n",
    "\n",
    "    print(f\"  {int(sparsity*100):>3}% sparsity | R²: {metrics['r2']:.4f} | RMSE: {metrics['rmse']:.6f}\")\n"
   ],
   "id": "eed5898d56108a05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Method 5 — Spearman Rank Correlation \n",
    "\n",
    "**Score:** `|SpearmanCorr(xᵢ(t), xⱼ(t))|`\n",
    "\n",
    "Measures how strongly two neurons co-activate at the same timestep using rank (non-linear, monotonic) correlation. Connections between weakly correlated neuron pairs are pruned first."
   ],
   "id": "972e62c8a7eadcad"
  },
  {
   "cell_type": "code",
   "id": "y9ptg39qbln",
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "# Spearman standalone: Score[i,j] = |SpearmanCorr(x_i(t), x_j(t))|\n",
    "states_ranked = np.zeros_like(states_train_ec)\n",
    "for col in range(N_NEURONS):\n",
    "    states_ranked[:, col] = rankdata(states_train_ec[:, col])\n",
    "\n",
    "# Contemporaneous Spearman correlation matrix\n",
    "spearman_contemp = np.corrcoef(states_ranked.T)\n",
    "spearman_contemp = np.nan_to_num(spearman_contemp, nan=0.0)\n",
    "\n",
    "# Score is absolute rank correlation at same timestep\n",
    "spearman_scores = np.abs(spearman_contemp)\n",
    "\n",
    "print(f\"Spearman standalone (contemporaneous) — active connections:\")\n",
    "print(f\"  Mean |ρ|: {spearman_scores[W_res != 0].mean():.4f}\")\n",
    "print(f\"  Max |ρ|:  {spearman_scores[W_res != 0].max():.4f}\")\n",
    "print(f\"  (no lag, no weight magnitude — pure static co-activation)\")\n",
    "\n",
    "spearman_results = {'removal_percentage': [], 'rmse_values': [], 'r2_values': []}\n",
    "\n",
    "for sparsity in SPARSITY_LEVELS:\n",
    "    W_res_sp, _ = prune_by_effective_connectivity(W_res, spearman_scores, sparsity)\n",
    "    node = QuantizedReservoirNode(W_res_sp, W_in, int_bias, name=\"quant_reservoir_spearman\")\n",
    "\n",
    "    states_train_pruned = node.run(int_x_train.astype(np.float64)) * qp['threshold_scale']\n",
    "    readout_use = Ridge(ridge=RIDGE)\n",
    "    readout_use.fit(states_train_pruned, y_train, warmup=WARMUP)\n",
    "\n",
    "    states_test_pruned = node.run(int_x_test.astype(np.float64)) * qp['threshold_scale']\n",
    "    y_pred = readout_use.run(states_test_pruned)\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred.reshape(-1, 1)\n",
    "\n",
    "    metrics = calculate_all_metrics_multi_dim(y_test_eval, y_pred)\n",
    "    spearman_results['removal_percentage'].append(int(sparsity * 100))\n",
    "    spearman_results['rmse_values'].append(metrics['rmse'])\n",
    "    spearman_results['r2_values'].append(metrics['r2'])\n",
    "\n",
    "    print(f\"  {int(sparsity*100):>3}% sparsity | R²: {metrics['r2']:.4f} | RMSE: {metrics['rmse']:.6f}\")\n"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Method 6 — Mutual Information (MI) Pruning\n\n**Score:** `MI(xᵢ(t), xⱼ(t))` — joint histogram-based mutual information between neuron pairs.\n\nMeasures the shared information between source and destination neuron activations using discretized joint histograms. Connections with low MI (neurons that share little predictable signal) are pruned first.\n\n**Note:** Scoring all connections is expensive, so only `MAX_WEIGHTS_TO_TEST` connections are scored; the rest receive the mean MI score as a neutral default.  \n**Unsupervised** — no task signal (y) is used.",
   "id": "f77ced146b188a9e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def collect_reservoir_activations_quantized(custom_node, input_data, W_matrix, threshold_scale, mi_bins, mi_max_weights):\n    quantized_states = custom_node.run(input_data.astype(np.float64)) * threshold_scale\n    non_zero_mask = W_matrix != 0.0\n    non_zero_indices = list(zip(*np.where(non_zero_mask)))\n    if len(non_zero_indices) > mi_max_weights:\n        non_zero_indices = non_zero_indices[:mi_max_weights]\n    paired_activation_data = {}\n    for idx, (i, j) in enumerate(non_zero_indices):\n        pre_activations  = quantized_states[:, i]\n        post_activations = quantized_states[:, j]\n        paired_activation_data[(i, j)] = (pre_activations.tolist(), post_activations.tolist())\n    return paired_activation_data, non_zero_indices\n\n\ndef calculate_mutual_information_saliencies(paired_activation_data, non_zero_indices, n_bins):\n    weight_saliencies_mi = []\n    weight_indices = []\n    for idx, (i, j) in enumerate(non_zero_indices):\n        pre_activations  = np.array(paired_activation_data[(i, j)][0])\n        post_activations = np.array(paired_activation_data[(i, j)][1])\n        joint_distribution, _, _ = np.histogram2d(pre_activations, post_activations, bins=n_bins)\n        mi = mutual_info_score(None, None, contingency=joint_distribution)\n        weight_saliencies_mi.append(mi)\n        weight_indices.append((i, j))\n    return weight_saliencies_mi, weight_indices\n\n\ndef mi_weight_removal_analysis_quantized(W_quantized, custom_node, X_train, y_train,\n                                         X_test, y_test, removal_percentages,\n                                         threshold_scale, mi_bins, quantization_bits):\n    all_active_indices = list(zip(*np.where(W_quantized != 0)))\n    total_active = len(all_active_indices)\n\n    paired_activation_data, scored_indices = collect_reservoir_activations_quantized(\n        custom_node, X_train, W_quantized, threshold_scale, mi_bins, MAX_WEIGHTS_TO_TEST\n    )\n    weight_saliencies_mi, weight_indices_mi = calculate_mutual_information_saliencies(\n        paired_activation_data, scored_indices, mi_bins\n    )\n\n    mean_mi = float(np.mean(weight_saliencies_mi)) if weight_saliencies_mi else 0.0\n    score_dict = {(i, j): s for (i, j), s in zip(weight_indices_mi, weight_saliencies_mi)}\n    all_scores = np.array([score_dict.get(ij, mean_mi) for ij in all_active_indices])\n    sorted_order = np.argsort(all_scores)\n\n    mi_results = {\n        'removal_percentage': [], 'weights_removed': [],\n        'mse_values': [], 'mae_values': [], 'rmse_values': [],\n        'nrmse_values': [], 'r2_values': []\n    }\n\n    for removal_pct in removal_percentages:\n        num_weights_to_remove = int((removal_pct / 100) * total_active)\n        W_modified = W_quantized.copy()\n        if num_weights_to_remove > 0:\n            for idx in sorted_order[:num_weights_to_remove]:\n                i, j = all_active_indices[idx]\n                W_modified[i, j] = 0\n\n        modified_node = copy.deepcopy(custom_node)\n        modified_node.set_param(\"Wr\", W_modified)\n\n        states_train_pruned = modified_node.run(X_train.astype(np.float64)) * threshold_scale\n        retrained_readout = Ridge(ridge=RIDGE)\n        retrained_readout.fit(states_train_pruned, y_train, warmup=WARMUP)\n\n        quantized_states_test = modified_node.run(X_test.astype(np.float64)) * threshold_scale\n        y_pred_modified = retrained_readout.run(quantized_states_test)\n        if y_pred_modified.ndim == 1:\n            y_pred_modified = y_pred_modified.reshape(-1, 1)\n        modified_metrics = calculate_all_metrics_multi_dim(y_test, y_pred_modified)\n\n        mi_results['removal_percentage'].append(removal_pct)\n        mi_results['weights_removed'].append(num_weights_to_remove)\n        mi_results['mse_values'].append(modified_metrics['mse'])\n        mi_results['mae_values'].append(modified_metrics['mae'])\n        mi_results['rmse_values'].append(modified_metrics['rmse'])\n        mi_results['nrmse_values'].append(modified_metrics['nrmse'])\n        mi_results['r2_values'].append(modified_metrics['r2'])\n\n    return mi_results\n\nmi_results = None\n\nif RUN_MI:\n    node = QuantizedReservoirNode(W_res, W_in, int_bias, name=\"quant_reservoir_mi\")\n    removal_percentages = [int(s * 100) for s in SPARSITY_LEVELS]\n\n    mi_results = mi_weight_removal_analysis_quantized(\n        W_res, node, int_x_train, y_train,\n        int_x_test, y_test_eval, removal_percentages,\n        qp['threshold_scale'], MI_BINS, QUANTIZATION_BITS\n    )\n",
   "id": "42e01b0fdfb0ea81",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **Comparison Plots**",
   "id": "52de2b01ccb4e49b"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# PRINT SUMMARY TABLE\n",
    "# ============================================================\n",
    "smart_methods = [(\"EC-Var\", attention_results), (\"PCA\", pca_results), (\"Random\", random_results)]\n",
    "if mi_results is not None:\n",
    "    smart_methods.append((\"Mutual Info\", mi_results))\n",
    "smart_methods.append((\"LASSO\",    lasso_results))\n",
    "smart_methods.append((\"Spearman\", spearman_results))\n",
    "\n",
    "all_pcts = attention_results['removal_percentage']\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"  RESULTS — {DATASET_NAME}, {QUANTIZATION_BITS}-bit, N={N_NEURONS}\")\n",
    "print(f\"  Baseline R²: {baseline_r2:.4f}  |  Baseline RMSE: {baseline_rmse:.6f}\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "col_w = 15\n",
    "header = f\"  {'Sparsity':>8} | \" + \" | \".join(f\"{n:^{col_w}}\" for n, _ in smart_methods)\n",
    "print(header)\n",
    "print(\"  \" + \"-\"*8 + \"-+-\" + (\"-+-\".join([\"-\"*col_w] * len(smart_methods))))\n",
    "\n",
    "for i, pct in enumerate(all_pcts):\n",
    "    r2_row = [res['r2_values'][i] if i < len(res['r2_values']) else float('-inf') for _, res in smart_methods]\n",
    "    best_r2 = max(r2_row)\n",
    "    cells = []\n",
    "    for j, (name, _) in enumerate(smart_methods):\n",
    "        star = \" ★\" if (r2_row[j] == best_r2 and pct != 0) else \"  \"\n",
    "        cells.append(f\"R²={r2_row[j]:>6.4f}{star}\")\n",
    "    print(f\"  {pct:>7}%  | \" + \" | \".join(cells))\n",
    "\n",
    "print(f\"\\n  (★ = winner at that sparsity level)\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "win_counts = {name: 0 for name, _ in smart_methods}\n",
    "n_pruned = 0\n",
    "for i, pct in enumerate(all_pcts):\n",
    "    if pct == 0:\n",
    "        continue\n",
    "    n_pruned += 1\n",
    "    best_val, best_name = None, None\n",
    "    for name, res in smart_methods:\n",
    "        val = res['r2_values'][i] if i < len(res['r2_values']) else float('-inf')\n",
    "        if best_val is None or val > best_val:\n",
    "            best_val, best_name = val, name\n",
    "    if best_name:\n",
    "        win_counts[best_name] += 1\n",
    "\n",
    "print(\"  Win counts (best R² at each pruned sparsity level):\")\n",
    "for name, cnt in win_counts.items():\n",
    "    bar = '█' * cnt + '░' * (n_pruned - cnt)\n",
    "    print(f\"    {name:>14}: {cnt}/{n_pruned}  {bar}\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# FIGURE: 1×2  —  R²  and  RMSE\n",
    "# ============================================================\n",
    "style_map = {\n",
    "    \"EC-Var\":      (\"tab:blue\",   \"o-\",  3.0),\n",
    "    \"PCA\":         (\"tab:orange\", \"s-.\", 2.5),\n",
    "    \"Mutual Info\": (\"tab:green\",  \"^:\",  2.0),\n",
    "    \"Random\":      (\"tab:red\",    \"x--\", 1.5),\n",
    "    \"LASSO\":       (\"tab:purple\", \"d-.\", 2.0),\n",
    "    \"Spearman\":    (\"tab:cyan\",   \"v:\",  2.0),\n",
    "}\n",
    "\n",
    "MAX_PCT = int(max(SPARSITY_LEVELS) * 100)\n",
    "\n",
    "def filter_to_max(res, max_pct):\n",
    "    x, r2, rmse_vals = [], [], []\n",
    "    for i, pct in enumerate(res['removal_percentage']):\n",
    "        if pct <= max_pct:\n",
    "            x.append(pct)\n",
    "            r2.append(res['r2_values'][i])\n",
    "            rmse_vals.append(res['rmse_values'][i])\n",
    "    return x, r2, rmse_vals\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\n",
    "    f\"Pruning Method Comparison  —  {DATASET_NAME}, {QUANTIZATION_BITS}-bit, N={N_NEURONS}\"\n",
    "    f\"   |   Baseline R² = {baseline_r2:.4f}\",\n",
    "    fontsize=14, fontweight='bold'\n",
    ")\n",
    "\n",
    "# R²\n",
    "ax = axes[0]\n",
    "for name, res in smart_methods:\n",
    "    c, sty, lw = style_map.get(name, (\"gray\", \"-\", 1.5))\n",
    "    x, r2, _ = filter_to_max(res, MAX_PCT)\n",
    "    r2_clipped = [max(v, 0.0) for v in r2]\n",
    "    ax.plot(x, r2_clipped, sty, label=name, color=c, linewidth=lw, markersize=9)\n",
    "ax.axhline(baseline_r2, color='black', linestyle=':', linewidth=1.5,\n",
    "           label=f'Baseline ({baseline_r2:.3f})')\n",
    "ax.set_xlim(-2, MAX_PCT + 2)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_xlabel('Sparsity (%)', fontsize=13)\n",
    "ax.set_ylabel('R²', fontsize=13)\n",
    "ax.set_title('R²  vs  Sparsity', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "ax = axes[1]\n",
    "for name, res in smart_methods:\n",
    "    c, sty, lw = style_map.get(name, (\"gray\", \"-\", 1.5))\n",
    "    x, _, rmse_vals = filter_to_max(res, MAX_PCT)\n",
    "    ax.plot(x, rmse_vals, sty, label=name, color=c, linewidth=lw, markersize=9)\n",
    "ax.axhline(baseline_rmse, color='black', linestyle=':', linewidth=1.5,\n",
    "           label=f'Baseline ({baseline_rmse:.4f})')\n",
    "ax.set_xlim(-2, MAX_PCT + 2)\n",
    "ax.set_xlabel('Sparsity (%)', fontsize=13)\n",
    "ax.set_ylabel('RMSE', fontsize=13)\n",
    "ax.set_title('RMSE  vs  Sparsity', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to plots/ folder\n",
    "plots_dir = \"plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "plot_filename = os.path.join(plots_dir, f\"comparison_{DATASET_TAG}_{QUANTIZATION_BITS}bit.png\")\n",
    "plt.savefig(plot_filename, dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Figure saved → {plot_filename}\")\n"
   ],
   "id": "bd62168a1618fa9b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
